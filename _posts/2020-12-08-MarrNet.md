---
layout: post
title: MarrNet
tags: [Computer Vision, 3D Shape Reconstruction, 2.5D Sketch]
gh-repo: jiajunwu/marrnet
gh-badge: [star, fork, follow]
mathjax: true
comments: true
---

* 论文名称：[MarrNet: 3D Shape Reconstruction via 2.5D Sketches](https://jiajunwu.com/papers/marrnet_nips.pdf)

* 论文作者：Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, William T. Freeman, Joshua B. Tenenbaum（MIT CSAIL, ShanghaiTech University）

* 收录情况：NeurIPS 2017

### 简介
（？？？为什么称作2.5D）

### 主要方法
![](../img/post/marrnet_fig2.png)

MarrNet 包含3个部分：
* 2.5D sketch estimator
* 3D shape estimator

* projection consistency function
    - 通过设计损失函数，使得估计的3D物体结构和推理的2.5D sketch保持一致

1. 2.5D Sketch Estimation
    - input: 2D rgb image
    - output: 2.5D sketch，由几个维度刻画
        * 物体表面法向量
        * （像素）深度值
        * 物体轮廓图 —— silhouette

    - 使用2.5D轮廓估计的目的是仅保留物体本身的特征（形状、深度、大小），去掉环境因素（光照、背景） —— **蒸馏**
    - 采用 encoder-decoder 架构进行 2.5D sketch 估计
        * 编码器是 ResNet-18，把256x256的图片，编码成512个8x8的特征图
        * 解码器包含4个这样的block：5x5 全卷积 + ReLU activation

2. 3D Shape Estimation
    - input: surface normal and depth image（这些东西从2.5D sketch得到）
    - output: 3D object

    - 因为有了2.5D sketch，所以这部分能在合成数据上训练，而且不用担心domain adaption问题，但不足之处是生成的3D object，真实感不足，一看就是生成出来的

    - 同样采用encoder-decoder结构
        - encoder: 5个block（conv + ReLU + pooling）$\rightarrow$ $2~fc~ layers$
            * 参考 Girdhar et al., 2016
            - 把输入数据编码成 200 维向量，给到decoder
        - decoder: 5个block（conv + ReLU）$\rightarrow$ $128 \times 128$ voxel-based reconstruction of the input
            * 参考 Wu et al., 2016b

3. Reprojection Consistency
    - 一些基于神经网络的方法，强制加入 估计的3D形状 和 2D表示 的一致性约束（一般是通过损失函数）
    - 本文也加入3D shape和2.5D sketch之间的一致性约束，共有两项
        * depth reprojection loss
        * surface normal reprojection loss
    - 一些形式化表示
        * $v_{x,y,z}$ 表示3D 体素网格中位置$(x,y,z)$对应的数值，且 $\forall~~ x,y,z~~ v_{x,y,z} \in [0,1]$ 成立
        * $d_{x,y}$ 表示位置$(x,y)$对应的深度值(？？？我理解这是说的像素值)
        * $n_{x,y} = (n_a, n_b, n_c)$ 表示估计的表面法向量

    - 本文有一个重要假设：认为从3D到2D的投影是正交的（我理解为主投影，原文是We assume orthographic projection in this work）

    - Depth reprojection loss
        - ![](../img/post/marrnet_fig3.png)
        * 这部分损失函数，设计目标是让体素值 $v_{x,y,d_{x,y}}$ 为1，因为此时深度值正好等于 $d_{x,y}$，深度小于 $d_{x,y}$ 时，体素值为0
        * 具体的损失函数如下
            - ![](../img/post/marrnet_eq1.png)
        * 损失函数对 $v_{x,y,z}$ 的导数
            - ![](../img/post/marrnet_eq2.png)
        - 当 $d_{x,y} = \infty$ 时，深度值进入一种特殊情况 —— 形状轮廓（？？？没理解这个因果关系）
            - 当线段与形状没有交集时，体素值应当全为0

    - Surface Normal reprojection loss
        - 向量 $n_x = (0, -n_c, n_b)$ 和 $n_y = (-n_c, 0， n_a)$与向量$n = (n_a, n_b, n_c)$正交，对前2个向量归一化得到
            - $n_x^' = (0, -1, n_b/n_c)$
            - $n_y^' = (-1, 0, n_a/n_c)$
            - 这两个向量位于点$(x,y,z)$对应的 surface plane（我理解为切面，因为前面说到了正交，而且是点$(x,y,z)$对应的平面，只能是切面）
        - 本损失函数试图保证 $(x,y,z) \pm n_x^'$ 和 $(x,y,z) \pm n_y^'$ 等于1，来匹配估计到的 surface normals
            - 上述约束仅在 目标体素 位于估计到的轮廓内部时适用（？？？不懂在说什么，道理是什么）
        - 令 $z = d_{x,y}$，Surface Normal reprojection loss 表示为
            - ![](../img/post/marrnet_eq3.png)
        - 损失函数的导数为
            - ![](../img/post/marrnet_eq4.png)

4. 训练模式
    - 两阶段训练模式
        - 第一阶段：训练2.5D sketch estimation module
        - 第二阶段：训练3D shape estimation module
        * 预训练的时候使用的都是 synthetic data，后来调优时候使用的是 real images
        - synthetic data 来自 ShapeNet objects 的合成图片，带有ground truth：surface normal, depth, silhouette annotations $\rightarrow$ L2 loss
        - 3D interpreter（我理解为3D shape estimation）在ground truth体素网格做监督 $\rightarrow$ cross entropy loss

    - 重投影一致性损失函数用来调优 3D shape estimation
        - 这部分输入是上一个模块的输出：normal, depth, and silhouette
        - 调优用的数据是 real images

    - 本文声称
        - 直接学习得到物体形状，能与2.5D sketch吻合较好，但是3D物体的外观真实性较差
        - 原因是跳过2.5D sketch，直接学习物体形状容易过拟合（并没有提供很多证据）

    - 在训练3D shape estimation module时，固定 decoder，仅调优encoder
    - 测试时，MarrNet能够自监督
        - e.g. 模型能在不带标注的单张图片 fine-tune（没明白这是在说什么，以及有什么作用）
        - 事实上，模型在每张图片迭代40次（为什么要用单张图片调优，我觉得这是一个平均数字吧 $\rightarrow$ 总共迭代次数 ÷ 图片数）
    
### 实验评估